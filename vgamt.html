<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="StyleSheet" href="style.css" type="text/css" media="all" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>VGAMT</title>
<style type="text/css">
#primarycontent h1 {
  font-variant: small-caps;
}
#primarycontent h3 {
}
#primarycontent teasertext {
  text-align: center;
}
#primarycontent p {
  text-align: center;
}
#primarycontent {
  text-align: justify;
}
#primarycontent p {
  text-align: justify;
  padding-left: 10px;
  padding-right: 10px;
}
#primarycontent p iframe {
  text-align: center;
}
.featart {
  margin:4px;
}
.hoverdiv {
  background-color:black;
  margin-top:2px;
  margin-bottom:10px;
  width:100%;
}
.hoverdiv:hover {
  background-color:white;
}
</style>

<script type="text/javascript">
  function togglevis(elid){
    el=document.getElementById(elid);
    aelid=elid+"a";
    ael=document.getElementById(aelid);
    if(el.style.display=='none'){
      el.style.display='inline-table';
      ael.innerHTML="[Hide BibTex]";
    }else{
      el.style.display='none';
      ael.innerHTML="[Show BibTex]";
    }
  }
</script>
<script type="text/javascript"
  src="http://www.maths.nottingham.ac.uk/personal/drw/LaTeXMathML.js">
</script>
<!--
<script type="text/javascript" src="http://math.etsu.edu/LaTeXMathML/LaTeXMathML.js"></script>
<link rel="stylesheet" type="text/css" href="http://math.etsu.edu/LaTeXMathML/LaTeXMathML.standardarticle.css" />
-->

<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-93210824-1', 'auto');
    ga('send', 'pageview');
</script>

<!-- using GIF onclick from https://www.hongkiat.com/blog/on-click-animated-gif/ -->
<link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/ionicons/1.5.2/css/ionicons.min.css">
<script src="http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="http://cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script>

</head>
<body>
<div id="primarycontent">
<h1 align="center" itemprop="name"><strong>Tackling Ambiguity with Images:<br/> Improved Multimodal Machine Translation and Contrastive Evaluation</strong></h1>

<center>
<ul id="people" itemprop="accountablePerson">
  <li><h4>
        <a href="https://mfuteral.github.io/">Matthieu Futeral</a><sup>1,2</sup>,
        <a href="https://www.di.ens.fr/willow/people_webpages/cordelia/">Cordelia Schmid</a><sup>1,2</sup>,
        <a href="https://www.di.ens.fr/~laptev/">Ivan Laptev</a><sup>1,2</sup>,
        <a href="https://pauillac.inria.fr/~sagot/">Benoît Sagot</a><sup>1</sup>,
        <a href="https://rbawden.github.io/">Rachel Bawden</a><sup>1</sup></h4></li>
  <li><sup>1</sup>Inria Paris</li>
  <li><sup>2</sup>Département d’informatique de l’ENS, CNRS, PSL Research University</li>
  <br />
  <li>
      <a href="https://arxiv.org/pdf/2212.10140.pdf">paper</a> /
      <a href="https://github.com/MatthieuFP/VGAMT">code</a> /
      <a href="https://github.com/MatthieuFP/CoMMuTE">data</a>
  </li>
</ul>
</center>

<h3 style="clear:both">Abstract</h3>
<p style="padding-left: 10px; padding-right: 10px;">
One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. 
  However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, 
  limited not only by the difficulty of building effective cross-modal representations, but also by the lack of specific evaluation and training data.
  We present a new MMT approach based on a strong text-only MT model, which uses neural adapters, a novel guided self-attention mechanism and which is 
  jointly trained on both visually-conditioned masking and MMT. We also introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation 
  set of ambiguous sentences and their possible translations, accompanied by disambiguating images corresponding to each translation. Our approach obtains
  competitive results compared to strong text-only models on standard English&#8594;French, English&#8594;German and English&#8594;Czech benchmarks 
  and outperforms baselines and state-of-the-art MMT systems by a large margin on our contrastive test set.
</p>

<h3 style="clear:both">Method: Visually Guided and Adapted Machine Translation system (VGAMT)</h3>
<div class="row">
    <p>
      <b>1. Exploiting large-scale text-only parallel corpus and English-only captioning data </b> <br><br>
      Multimodal parallel datasets are expensive to obtain as it requires a sentence, its translation and an additional image. MMT models are therefore trained on a few amount of data and performs poorly in comparison to state-of-the-art MT systems trained on large-scale parallel corpus.
      Here, we propose to exploit the large amount of existing text-only parallel data by adapting a frozen MT model to multimodal inputs with lightweight modules. In addition to the classic multimodal machine translation (MMT) objective, we also train our model on visually conditioned masked language modeling (VMLM) objective and English captioning data to increase the number of text-image pairs during training.
    </p>
    <p>
      <center>
        <img src="images/main_fig_bix_times_six.jpeg" height="360px" />
      </center>
    </p>
</div>
<div class="row">
    <p>
      <b>2. Guided self-attention: effective exploitation of visual features into MT systems.</b> <br><br>
    </p>
    <p>
      <center>
        <!-- <figure> -->
          <!-- <img src="img/example_glasses.jpg" height="140px"> -->
        <!-- </figure> -->
        <!-- <iframe width="380" height="300" src="https://www.youtube.com/embed/EnTiwiX8Xz8?rel=0&modestbranding=1&autohide=1&showinfo=0&controls=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
      </center>
    </p>
</div>


<h3 style="clear:both">Contrastive Multilingual Multimodal Translation Evaluation dataset (CoMMuTE)</h3>
  <div class="row">
     <div class="column">
       <p>
       Traditional MMT benchmarks contain only a handful ambiguous examples where images are necessary to produce accurate translation of the source sentence. To handle this, we introduce CoMMuTE, a contrastive evaluation dataset, composed of ambiguous examples accompanied with images and two translations: a correct one and an incorrect one.
        The additional image determines which translation is the correct one. Models are asked to rank translations based on the source sentence and the image. On the example on the right, `mole' can either be translated by `grain de beauté' or `taupe' in French depending on the conditional image.
       </p>
    </div>
    <div class="column">
      <center>
      <p>
        <img src="images/commute_example.png", height="250px" />
      </p>
      </center>
    </div>
  </div>

<h3 style="clear:both">Translation examples</h3>
  <div class="column">
    <p>
      <img src="images/example_glasses.jpg", height="250px" />
    </p>
  </div>
  <div class="column">
    <img src="images/", height="250px" />
  </div>
  <div class="column">
    <img src="images/", height="250px" />
  </div>
  

<!--<h3 style="clear:both">Code</h3>
<p style="padding-left: 10px; padding-right: 10px;">
The visually diverse environments are available at <a href="https://github.com/vitchyr/multiworld">github.com/vitchyr/multiworld</a>. Algorithm code is available at <a href="https://github.com/anair13/rlkit/tree/ccrig/examples/ccrig">github.com/anair13/rlkit/tree/ccrig/examples/ccrig</a>.
</p>-->

<h3 style="clear:both">Data</h3>
<p style="padding-left: 10px; padding-right: 10px;">
Our contrastive evaluation dataset CoMMuTE is available at this <a href="https://github.com/MatthieuFP/CoMMuTE">link</a>.
</p>

<h3 style="clear:both">Citation</h3>
<p style="padding-left: 10px; padding-right: 10px;">
<code>
  @article{vgamt, <br />
author&nbsp;&nbsp;&nbsp;&nbsp;= {Futeral, Matthieu and Schmid, Cordelia and Laptev, Ivan and Sagot, Benoît and Bawden, Rachel}, <br />
title&nbsp;&nbsp;= {Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation}, <br />
journal&nbsp;= {arXiv preprint arXiv:2212.10140}, <br />
year&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= {2022} <br />
  }
</code>
</p>

<script src="js/script-min.js"></script>


<!-- <h3 style="clear:both">Website Template</h3>
<p style="padding-left: 10px; padding-right: 10px;">
The template for this website has been adopted from Carl Doersch.
</p> -->


</body>
</html>
