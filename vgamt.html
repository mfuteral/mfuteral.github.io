<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="StyleSheet" href="style.css" type="text/css" media="all" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>VGAMT</title>
<style type="text/css">
#primarycontent h1 {
  font-variant: small-caps;
}
#primarycontent h3 {
}
#primarycontent teasertext {
  text-align: center;
}
#primarycontent p {
  text-align: center;
}
#primarycontent {
  text-align: justify;
}
#primarycontent p {
  text-align: justify;
  padding-left: 10px;
  padding-right: 10px;
}
#primarycontent p iframe {
  text-align: center;
}
.featart {
  margin:4px;
}
.hoverdiv {
  background-color:black;
  margin-top:2px;
  margin-bottom:10px;
  width:100%;
}
.hoverdiv:hover {
  background-color:white;
}
</style>

<script type="text/javascript">
  function togglevis(elid){
    el=document.getElementById(elid);
    aelid=elid+"a";
    ael=document.getElementById(aelid);
    if(el.style.display=='none'){
      el.style.display='inline-table';
      ael.innerHTML="[Hide BibTex]";
    }else{
      el.style.display='none';
      ael.innerHTML="[Show BibTex]";
    }
  }
</script>
<script type="text/javascript"
  src="http://www.maths.nottingham.ac.uk/personal/drw/LaTeXMathML.js">
</script>
<!--
<script type="text/javascript" src="http://math.etsu.edu/LaTeXMathML/LaTeXMathML.js"></script>
<link rel="stylesheet" type="text/css" href="http://math.etsu.edu/LaTeXMathML/LaTeXMathML.standardarticle.css" />
-->

<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-93210824-1', 'auto');
    ga('send', 'pageview');
</script>

<!-- using GIF onclick from https://www.hongkiat.com/blog/on-click-animated-gif/ -->
<link rel="stylesheet" href="http://cdnjs.cloudflare.com/ajax/libs/ionicons/1.5.2/css/ionicons.min.css">
<script src="http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="http://cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script>

</head>
<body>
<div id="primarycontent">
<h1 align="center" itemprop="name"><strong>Tackling Ambiguity with Images:<br/> Improved Multimodal Machine Translation and Contrastive Evaluation</strong></h1>

<center>
<ul id="people" itemprop="accountablePerson">
  <li><h4>
        <a href="https://mfuteral.github.io/">Matthieu Futeral</a><sup>1,2</sup>,
        <a href="https://www.di.ens.fr/willow/people_webpages/cordelia/">Cordelia Schmid</a><sup>1,2</sup>,
        <a href="https://www.di.ens.fr/~laptev/">Ivan Laptev</a><sup>1,2</sup>,
        <a href="https://pauillac.inria.fr/~sagot/">Benoît Sagot</a><sup>1</sup>,
        <a href="https://rbawden.github.io/">Rachel Bawden</a><sup>1</sup></h4></li>
  <li><sup>1</sup>Inria Paris</li>
  <li><sup>2</sup>Département d’informatique de l’ENS, CNRS, PSL Research University</li>
  <br />
  <li>
      <a href="https://arxiv.org/pdf/2212.10140.pdf">paper</a> /
      <!-- <a href="https://github.com/anair13/rlkit/tree/ccrig/examples/ccrig">code</a> / -->
      <a href="https://github.com/MatthieuFP/CoMMuTE">data</a>
  </li>
</ul>
</center>

<h3 style="clear:both">Abstract</h3>
<p style="padding-left: 10px; padding-right: 10px;">
One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. 
  However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, 
  limited not only by the difficulty of building effective cross-modal representations, but also by the lack of specific evaluation and training data.
  We present a new MMT approach based on a strong text-only MT model, which uses neural adapters, a novel guided self-attention mechanism and which is 
  jointly trained on both visually-conditioned masking and MMT. We also introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation 
  set of ambiguous sentences and their possible translations, accompanied by disambiguating images corresponding to each translation. Our approach obtains
  competitive results compared to strong text-only models on standard English&#8594;French, English&#8594;German and English&#8594;Czech benchmarks 
  and outperforms baselines and state-of-the-art MMT systems by a large margin on our contrastive test set.
</p>

<h3 style="clear:both">Method: Visually Guided and Adapted Machine Translation system (VGAMT)</h3>
<div class="row">
    <p>
      1. The robot collects random interaction data, to be used for training a representation and as off-policy data for RL.
    </p>
    <p>
      <center>
        <img src="img/method_step1.png" height="140px" />
      </center>
    </p>
</div>
<div class="row">
    <p>
      3. At training time, the robot learns a policy with RL to minimize the latent distance to a generated goal.
    </p>
    <p>
      <center>
        <!-- <figure> -->
          <!-- <img src="img/robot_test.png" height="140px"> -->
        <!-- </figure> -->
        <iframe width="380" height="300" src="https://www.youtube.com/embed/EnTiwiX8Xz8?rel=0&modestbranding=1&autohide=1&showinfo=0&controls=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </center>
    </p>
</div>


<h3 style="clear:both">Contrastive Multilingual Multimodal Translation Evaluation dataset (CoMMuTE)</h3>
<p style="padding-left: 10px; padding-right: 10px;">
  <center>
    <!-- <iframe src="https://drive.google.com/file/d/1Wj4oqLVLk2Lix2IGD0yP5avBKZGIZMfg/preview" width="640" height="480"></iframe> -->
    <iframe width="560" height="315" src="https://www.youtube.com/embed/Cqp_phACou0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </center>
</p>

<!--<h3 style="clear:both">Code</h3>
<p style="padding-left: 10px; padding-right: 10px;">
The visually diverse environments are available at <a href="https://github.com/vitchyr/multiworld">github.com/vitchyr/multiworld</a>. Algorithm code is available at <a href="https://github.com/anair13/rlkit/tree/ccrig/examples/ccrig">github.com/anair13/rlkit/tree/ccrig/examples/ccrig</a>.
</p>-->

<h3 style="clear:both">Data</h3>
<p style="padding-left: 10px; padding-right: 10px;">
Our contrastive evaluation dataset CoMMuTE is available at this <a href="https://github.com/MatthieuFP/CoMMuTE">link</a>.
</p>

<h3 style="clear:both">Citation</h3>
<p style="padding-left: 10px; padding-right: 10px;">
<code>
  @article{vgamt, <br />
&nbsp;&nbsp;author&nbsp;&nbsp;&nbsp;&nbsp;= {Futeral, Matthieu and Schmid, Cordelia and Laptev, Ivan and Sagot, Benoît and Bawden, Rachel}, <br />
&nbsp;title&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= {Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation}, <br />
&nbsp;&nbsp;publisher&nbsp;= {arXiv}, <br />
&nbsp;&nbsp;year&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= {2022} <br />
  }
</code>
</p>

<script src="js/script-min.js"></script>


<!-- <h3 style="clear:both">Website Template</h3>
<p style="padding-left: 10px; padding-right: 10px;">
The template for this website has been adopted from Carl Doersch.
</p> -->


</body>
</html>
